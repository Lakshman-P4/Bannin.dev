{
  "_version": "2026-02-21",
  "_note": "Default platform limits, LLM pricing, danger zones, and analytics config. Updated via remote config when available.",

  "colab": {
    "tiers": {
      "free":  {"max_session_hours": 12, "idle_timeout_min": 90,  "disk_gb": 110, "ram_gb": 13, "concurrent_sessions": 2, "background_execution": false},
      "pro":   {"max_session_hours": 24, "idle_timeout_min": 180, "disk_gb": 150, "ram_gb": 26, "concurrent_sessions": 3, "background_execution": false},
      "pro+":  {"max_session_hours": 24, "idle_timeout_min": 180, "disk_gb": 200, "ram_gb": 53, "concurrent_sessions": 5, "background_execution": true}
    },
    "gpu_specs": {
      "T4":   {"vram_gb": 15,   "architecture": "Turing",       "cu_per_hour": 1.96},
      "L4":   {"vram_gb": 22.5, "architecture": "Ada Lovelace", "cu_per_hour": 4.82},
      "A100": {"vram_gb": 40,   "architecture": "Ampere",       "cu_per_hour": 13.08},
      "V100": {"vram_gb": 16,   "architecture": "Volta",        "cu_per_hour": 5.0, "retired": true},
      "P100": {"vram_gb": 16,   "architecture": "Pascal",       "cu_per_hour": 1.96, "retired": true}
    },
    "limits": {
      "max_notebook_size_mb": 20,
      "max_file_upload_gb": 2,
      "max_file_download_mb": 100
    }
  },

  "kaggle": {
    "session_limits_hours": {
      "cpu": 12,
      "gpu": 9,
      "tpu": 9
    },
    "idle_timeout_min": 60,
    "quotas": {
      "gpu_weekly_hours": 30,
      "tpu_weekly_hours": 20
    },
    "gpu_specs": {
      "P100": {"vram_gb": 16, "memory_type": "HBM2",  "bandwidth_gbps": 732},
      "T4":   {"vram_gb": 16, "memory_type": "GDDR6", "bandwidth_gbps": 320}
    },
    "storage": {
      "output_limit_gb": 20,
      "output_file_limit": 500,
      "input_limit_gb": 100,
      "notebook_source_limit_mb": 1
    },
    "limits": {
      "max_concurrent_gpu_sessions": 1,
      "dataset_size_limit_gb": 100,
      "file_upload_web_mb": 500,
      "file_upload_api_gb": 2
    },
    "ram_gb": {
      "cpu": 30,
      "gpu": 29,
      "tpu": 16
    },
    "cpu_cores": 4
  },

  "llm": {
    "_note": "LLM model pricing (per 1M tokens, USD) and context windows. Updated via remote config as prices change.",
    "models": {
      "gpt-4o":                      {"provider": "openai",    "input_per_m": 2.50,  "output_per_m": 10.00, "cached_input_per_m": 1.25,  "context_window": 128000, "danger_zone_percent": 82},
      "gpt-4o-mini":                 {"provider": "openai",    "input_per_m": 0.15,  "output_per_m": 0.60,  "cached_input_per_m": 0.075, "context_window": 128000, "danger_zone_percent": 82},
      "gpt-4-turbo":                 {"provider": "openai",    "input_per_m": 10.00, "output_per_m": 30.00, "cached_input_per_m": 10.00, "context_window": 128000, "danger_zone_percent": 80},
      "gpt-4":                       {"provider": "openai",    "input_per_m": 30.00, "output_per_m": 60.00, "cached_input_per_m": 30.00, "context_window": 8192,   "danger_zone_percent": 75},
      "gpt-3.5-turbo":               {"provider": "openai",    "input_per_m": 0.50,  "output_per_m": 1.50,  "cached_input_per_m": 0.50,  "context_window": 16385,  "danger_zone_percent": 75},
      "o1":                          {"provider": "openai",    "input_per_m": 15.00, "output_per_m": 60.00, "cached_input_per_m": 7.50,  "context_window": 200000, "danger_zone_percent": 80},
      "o1-mini":                     {"provider": "openai",    "input_per_m": 3.00,  "output_per_m": 12.00, "cached_input_per_m": 1.50,  "context_window": 128000, "danger_zone_percent": 82},
      "o3-mini":                     {"provider": "openai",    "input_per_m": 1.10,  "output_per_m": 4.40,  "cached_input_per_m": 0.55,  "context_window": 200000, "danger_zone_percent": 80},

      "claude-opus-4-6":             {"provider": "anthropic", "input_per_m": 5.00,  "output_per_m": 25.00, "cached_input_per_m": 0.50,  "context_window": 200000, "danger_zone_percent": 85},
      "claude-opus-4-5":             {"provider": "anthropic", "input_per_m": 5.00,  "output_per_m": 25.00, "cached_input_per_m": 0.50,  "context_window": 200000, "danger_zone_percent": 85},
      "claude-opus-4-20250514":      {"provider": "anthropic", "input_per_m": 15.00, "output_per_m": 75.00, "cached_input_per_m": 1.50,  "context_window": 200000, "danger_zone_percent": 85},
      "claude-sonnet-4-6":           {"provider": "anthropic", "input_per_m": 3.00,  "output_per_m": 15.00, "cached_input_per_m": 0.30,  "context_window": 200000, "danger_zone_percent": 87},
      "claude-sonnet-4-5":           {"provider": "anthropic", "input_per_m": 3.00,  "output_per_m": 15.00, "cached_input_per_m": 0.30,  "context_window": 200000, "danger_zone_percent": 87},
      "claude-sonnet-4-20250514":    {"provider": "anthropic", "input_per_m": 3.00,  "output_per_m": 15.00, "cached_input_per_m": 0.30,  "context_window": 200000, "danger_zone_percent": 87},
      "claude-3-5-sonnet-20241022":  {"provider": "anthropic", "input_per_m": 3.00,  "output_per_m": 15.00, "cached_input_per_m": 0.30,  "context_window": 200000, "danger_zone_percent": 85},
      "claude-haiku-4-5-20251001":   {"provider": "anthropic", "input_per_m": 1.00,  "output_per_m": 5.00,  "cached_input_per_m": 0.10,  "context_window": 200000, "danger_zone_percent": 87},
      "claude-3-5-haiku-20241022":   {"provider": "anthropic", "input_per_m": 0.80,  "output_per_m": 4.00,  "cached_input_per_m": 0.08,  "context_window": 200000, "danger_zone_percent": 85},
      "claude-3-haiku-20240307":     {"provider": "anthropic", "input_per_m": 0.25,  "output_per_m": 1.25,  "cached_input_per_m": 0.03,  "context_window": 200000, "danger_zone_percent": 85},

      "gemini-2.5-pro":              {"provider": "google",    "input_per_m": 1.25,  "output_per_m": 10.00, "cached_input_per_m": 0.315, "context_window": 1048576, "danger_zone_percent": 50},
      "gemini-2.5-flash":            {"provider": "google",    "input_per_m": 0.30,  "output_per_m": 2.50,  "cached_input_per_m": 0.075, "context_window": 1048576, "danger_zone_percent": 55},
      "gemini-2.0-flash":            {"provider": "google",    "input_per_m": 0.10,  "output_per_m": 0.40,  "cached_input_per_m": 0.025, "context_window": 1048576, "danger_zone_percent": 55},
      "gemini-1.5-pro":              {"provider": "google",    "input_per_m": 1.25,  "output_per_m": 5.00,  "cached_input_per_m": 0.315, "context_window": 2097152, "danger_zone_percent": 50},
      "gemini-1.5-flash":            {"provider": "google",    "input_per_m": 0.075, "output_per_m": 0.30,  "cached_input_per_m": 0.019, "context_window": 1048576, "danger_zone_percent": 55},

      "grok-2":                      {"provider": "xai",       "input_per_m": 2.00,  "output_per_m": 10.00, "cached_input_per_m": 2.00,  "context_window": 131072, "danger_zone_percent": 80},
      "grok-3":                      {"provider": "xai",       "input_per_m": 3.00,  "output_per_m": 15.00, "cached_input_per_m": 3.00,  "context_window": 131072, "danger_zone_percent": 80},
      "grok-3-mini":                 {"provider": "xai",       "input_per_m": 0.30,  "output_per_m": 0.50,  "cached_input_per_m": 0.30,  "context_window": 131072, "danger_zone_percent": 80},

      "mistral-large-latest":        {"provider": "mistral",   "input_per_m": 2.00,  "output_per_m": 6.00,  "cached_input_per_m": 2.00,  "context_window": 128000, "danger_zone_percent": 80},
      "mistral-small-latest":        {"provider": "mistral",   "input_per_m": 0.20,  "output_per_m": 0.60,  "cached_input_per_m": 0.20,  "context_window": 128000, "danger_zone_percent": 80},
      "codestral-latest":            {"provider": "mistral",   "input_per_m": 0.30,  "output_per_m": 0.90,  "cached_input_per_m": 0.30,  "context_window": 256000, "danger_zone_percent": 75},

      "llama-3.3-70b":               {"provider": "meta",      "input_per_m": 0.60,  "output_per_m": 0.60,  "cached_input_per_m": 0.60,  "context_window": 128000, "danger_zone_percent": 65},
      "llama-3.1-405b":              {"provider": "meta",      "input_per_m": 3.50,  "output_per_m": 3.50,  "cached_input_per_m": 3.50,  "context_window": 128000, "danger_zone_percent": 65},
      "llama-3.1-8b":                {"provider": "meta",      "input_per_m": 0.10,  "output_per_m": 0.10,  "cached_input_per_m": 0.10,  "context_window": 128000, "danger_zone_percent": 65},

      "command-r-plus":              {"provider": "cohere",    "input_per_m": 2.50,  "output_per_m": 10.00, "cached_input_per_m": 2.50,  "context_window": 128000, "danger_zone_percent": 75},
      "command-r":                   {"provider": "cohere",    "input_per_m": 0.15,  "output_per_m": 0.60,  "cached_input_per_m": 0.15,  "context_window": 128000, "danger_zone_percent": 75}
    },
    "default_danger_zone_percent": 65
  },

  "intelligence": {
    "_note": "Phase 2 intelligence engine configuration. Controls metric history, OOM prediction, progress detection, alerts, and health scoring.",
    "collection_interval_seconds": 4,
    "history_max_readings": 450,

    "oom": {
      "min_data_points": 12,
      "confidence_threshold": 70,
      "ram_critical_percent": 95,
      "gpu_critical_percent": 95
    },

    "progress": {
      "stdout_patterns": [
        {"name": "epoch",   "regex": "Epoch\\s+(\\d+)[/](\\d+)", "current_group": 1, "total_group": 2},
        {"name": "step",    "regex": "Step\\s+(\\d+)[/](\\d+)",  "current_group": 1, "total_group": 2},
        {"name": "batch",   "regex": "Batch\\s+(\\d+)[/](\\d+)", "current_group": 1, "total_group": 2},
        {"name": "percent", "regex": "(\\d+)%",                  "current_group": 1, "total_group": null},
        {"name": "counter", "regex": "(\\d+)[/](\\d+)",          "current_group": 1, "total_group": 2}
      ],
      "stall_timeout_seconds": 300
    },

    "training_detection": {
      "_note": "Detects ML training processes from command-line inspection during background process scans.",
      "scripts": [
        "train\\.py", "train_\\w+", "finetune\\w*", "fine_tune\\w*",
        "run_clm\\.py", "run_mlm\\.py", "run_glue\\.py", "trainer\\.py",
        "run_training\\.py", "run_train\\.py"
      ],
      "arg_keywords": [
        "train", "training", "fit", "finetune", "fine_tune",
        "--do_train", "--num_train_epochs", "epochs"
      ],
      "frameworks": [
        "transformers", "pytorch_lightning", "keras", "tensorflow",
        "accelerate", "deepspeed", "fairseq", "torch.distributed",
        "lightning", "detectron2"
      ],
      "finished_ttl_seconds": 300,
      "max_tracked": 100
    },

    "alerts": {
      "rules": [
        {
          "id": "ram_high",
          "metric": "memory.percent",
          "operator": ">=",
          "threshold": 80,
          "severity": "warning",
          "message": "RAM HIGH: {value}% used",
          "cooldown_seconds": 120,
          "platforms": ["all"]
        },
        {
          "id": "ram_critical",
          "metric": "memory.percent",
          "operator": ">=",
          "threshold": 95,
          "severity": "critical",
          "message": "RAM CRITICAL: {value}% used - system may crash",
          "cooldown_seconds": 60,
          "platforms": ["all"]
        },
        {
          "id": "disk_warning",
          "metric": "disk.percent",
          "operator": ">=",
          "threshold": 80,
          "severity": "warning",
          "message": "DISK WARNING: {value}% used",
          "cooldown_seconds": 300,
          "platforms": ["all"]
        },
        {
          "id": "disk_critical",
          "metric": "disk.percent",
          "operator": ">=",
          "threshold": 95,
          "severity": "critical",
          "message": "DISK CRITICAL: {value}% used - may run out of space",
          "cooldown_seconds": 120,
          "platforms": ["all"]
        },
        {
          "id": "gpu_vram_warning",
          "metric": "gpu.memory_percent",
          "operator": ">=",
          "threshold": 80,
          "severity": "warning",
          "message": "GPU VRAM HIGH: {value}% used",
          "cooldown_seconds": 120,
          "platforms": ["all"]
        },
        {
          "id": "gpu_vram_critical",
          "metric": "gpu.memory_percent",
          "operator": ">=",
          "threshold": 95,
          "severity": "critical",
          "message": "GPU VRAM CRITICAL: {value}% - OOM risk",
          "cooldown_seconds": 60,
          "platforms": ["all"]
        },
        {
          "id": "cpu_sustained_high",
          "metric": "cpu.percent",
          "operator": ">=",
          "threshold": 90,
          "severity": "warning",
          "message": "CPU SUSTAINED HIGH: {value}% usage",
          "cooldown_seconds": 180,
          "platforms": ["all"]
        },
        {
          "id": "oom_predicted",
          "metric": "predictions.oom.ram.minutes_until_full",
          "operator": "<=",
          "threshold": 10,
          "condition": "predictions.oom.ram.confidence >= 70",
          "severity": "critical",
          "message": "OOM PREDICTED: RAM will be full in ~{value} minutes",
          "cooldown_seconds": 60,
          "platforms": ["all"]
        },
        {
          "id": "gpu_oom_predicted",
          "metric": "predictions.oom.gpu.minutes_until_full",
          "operator": "<=",
          "threshold": 5,
          "condition": "predictions.oom.gpu.confidence >= 70",
          "severity": "critical",
          "message": "GPU OOM PREDICTED: VRAM will be full in ~{value} minutes",
          "cooldown_seconds": 30,
          "platforms": ["all"]
        },
        {
          "id": "session_expiring",
          "metric": "platform.session.remaining_seconds",
          "operator": "<=",
          "threshold": 900,
          "severity": "warning",
          "message": "SESSION EXPIRING: {value_human} remaining",
          "cooldown_seconds": 120,
          "platforms": ["colab", "kaggle"]
        },
        {
          "id": "session_expiring_critical",
          "metric": "platform.session.remaining_seconds",
          "operator": "<=",
          "threshold": 300,
          "severity": "critical",
          "message": "SESSION CRITICAL: Only {value_human} remaining!",
          "cooldown_seconds": 60,
          "platforms": ["colab", "kaggle"]
        },
        {
          "id": "gpu_quota_low",
          "metric": "platform.gpu_quota.remaining_hours",
          "operator": "<=",
          "threshold": 2,
          "severity": "warning",
          "message": "GPU QUOTA LOW: Only {value} hours remaining this week",
          "cooldown_seconds": 600,
          "platforms": ["kaggle"]
        },
        {
          "id": "storage_nearly_full",
          "metric": "platform.storage.percent",
          "operator": ">=",
          "threshold": 80,
          "severity": "warning",
          "message": "STORAGE WARNING: {value}% used on platform storage",
          "cooldown_seconds": 300,
          "platforms": ["colab", "kaggle"]
        },
        {
          "id": "context_window_warning",
          "metric": "llm.context_percent",
          "operator": ">=",
          "threshold": 75,
          "severity": "warning",
          "message": "CONTEXT WINDOW: {value}% used - conversation getting long",
          "cooldown_seconds": 120,
          "platforms": ["all"]
        },
        {
          "id": "context_window_critical",
          "metric": "llm.context_percent",
          "operator": ">=",
          "threshold": 90,
          "severity": "critical",
          "message": "CONTEXT WINDOW CRITICAL: {value}% used - consider starting a new chat",
          "cooldown_seconds": 60,
          "platforms": ["all"]
        },
        {
          "id": "latency_degrading",
          "metric": "llm.latency_ratio",
          "operator": ">=",
          "threshold": 1.5,
          "severity": "warning",
          "message": "LATENCY DEGRADING: Responses are {value}x slower than earlier",
          "cooldown_seconds": 180,
          "platforms": ["all"]
        },
        {
          "id": "llm_spend_high",
          "metric": "llm.total_cost_usd",
          "operator": ">=",
          "threshold": 10.0,
          "severity": "warning",
          "message": "LLM SPEND: ${value} spent this session",
          "cooldown_seconds": 600,
          "platforms": ["all"]
        },
        {
          "id": "training_complete",
          "metric": "tasks.all_completed",
          "operator": "==",
          "threshold": true,
          "severity": "info",
          "message": "TRAINING COMPLETE: All tracked tasks have finished",
          "cooldown_seconds": 0,
          "platforms": ["all"]
        },
        {
          "id": "eta_exceeds_session",
          "metric": "tasks.longest_eta_seconds",
          "compare_to": "platform.session.remaining_seconds",
          "operator": ">",
          "severity": "critical",
          "message": "WILL NOT FINISH: Training ETA exceeds session time remaining",
          "cooldown_seconds": 120,
          "platforms": ["colab", "kaggle"]
        },
        {
          "id": "conversation_health_low",
          "metric": "llm.health_score",
          "operator": "<=",
          "threshold": 40,
          "severity": "warning",
          "message": "CONVERSATION HEALTH LOW: Score {value}/100 - consider starting a new chat",
          "cooldown_seconds": 300,
          "platforms": ["all"]
        },
        {
          "id": "conversation_health_critical",
          "metric": "llm.health_score",
          "operator": "<=",
          "threshold": 20,
          "severity": "critical",
          "message": "CONVERSATION HEALTH CRITICAL: Score {value}/100 - this chat is severely degraded",
          "cooldown_seconds": 120,
          "platforms": ["all"]
        },
        {
          "id": "ollama_vram_high",
          "metric": "ollama.vram_pressure",
          "operator": ">=",
          "threshold": 80,
          "severity": "warning",
          "message": "OLLAMA VRAM HIGH: {value}% of model VRAM allocation used",
          "cooldown_seconds": 120,
          "platforms": ["all"]
        },
        {
          "id": "mcp_session_fatigue",
          "metric": "mcp.session_fatigue",
          "operator": ">=",
          "threshold": 60,
          "severity": "warning",
          "message": "MCP SESSION FATIGUE: Context quality likely degrading (fatigue score {value})",
          "cooldown_seconds": 300,
          "platforms": ["all"]
        }
      ]
    },

    "conversation_health": {
      "weight_profiles": {
        "api": {
          "context_freshness": 0.45,
          "latency_health": 0.30,
          "cost_efficiency": 0.25,
          "session_fatigue": 0.0,
          "tool_call_burden": 0.0,
          "vram_pressure": 0.0,
          "inference_throughput": 0.0
        },
        "mcp": {
          "context_freshness": 0.25,
          "latency_health": 0.15,
          "cost_efficiency": 0.0,
          "session_fatigue": 0.35,
          "tool_call_burden": 0.25,
          "vram_pressure": 0.0,
          "inference_throughput": 0.0
        },
        "local_llm": {
          "context_freshness": 0.30,
          "latency_health": 0.30,
          "cost_efficiency": 0.0,
          "session_fatigue": 0.0,
          "tool_call_burden": 0.0,
          "vram_pressure": 0.25,
          "inference_throughput": 0.15
        }
      },
      "thresholds": {
        "excellent": 90,
        "good": 70,
        "fair": 50,
        "poor": 30
      }
    }
  },

  "ollama": {
    "host": "http://localhost:11434",
    "poll_interval_seconds": 15
  },

  "analytics": {
    "db_path": null,
    "max_queue_size": 10000,
    "flush_interval_seconds": 2,
    "prune_after_days": 30,
    "downsample_metric_interval_seconds": 300
  },

  "mcp": {
    "_note": "MCP server configuration for AI tool integration.",
    "enabled_tools": [
      "get_system_metrics",
      "get_training_status",
      "predict_oom",
      "get_active_alerts",
      "get_llm_usage",
      "check_context_health",
      "get_recommendations",
      "query_history",
      "search_events"
    ],
    "log_tool_calls": true
  }
}
